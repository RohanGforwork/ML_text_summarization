{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME18CohqL7WRy/hMy8JdY6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohanGforwork/ML_text_summarization/blob/main/Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary Modules!!!\n",
        "!pip install googletrans==4.0.0-rc1 better_profanity nltk\n",
        "!pip install wordfilter\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cq3mPgjhPiGc",
        "outputId": "32d9bcaf-e8fd-4e62-c5d7-a8d3489331be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting better_profanity\n",
            "  Downloading better_profanity-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=367fefe692d5bace7e80b2819570e64feef777780c223c7d44c19d2a11dcf7e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, better_profanity, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.8 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.61.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed better_profanity-0.7.0 chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              },
              "id": "54136f82a6464ceeb7fec4fd9f52b36e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordfilter\n",
            "  Downloading wordfilter-0.2.7-py3-none-any.whl.metadata (3.2 kB)\n",
            "Downloading wordfilter-0.2.7-py3-none-any.whl (4.4 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language translation\n"
      ],
      "metadata": {
        "id": "ybKSmeDxPMF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from googletrans import Translator\n",
        "from scipy.special import softmax\n",
        "import re\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self, model_name=\"cardiffnlp/twitter-roberta-base-sentiment\"):\n",
        "        \"\"\"\n",
        "        Initializes the translator and sentiment analyzer.\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        self.translator = Translator()\n",
        "        self.labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "    def detect_and_translate(self, text):\n",
        "        \"\"\"\n",
        "        Detects language and translates text to English if necessary.\n",
        "        \"\"\"\n",
        "        detected = self.translator.detect(text)\n",
        "        detected_lang = detected.lang\n",
        "        confidence = round(detected.confidence, 2) if detected.confidence else 1.0\n",
        "\n",
        "        if detected_lang != \"en\":\n",
        "            translated_text = self.translator.translate(text, dest=\"en\").text\n",
        "        else:\n",
        "            translated_text = text\n",
        "\n",
        "        return {\n",
        "            \"detected_language\": detected_lang,\n",
        "            \"confidence\": confidence,\n",
        "            \"translated_text\": translated_text\n",
        "        }\n",
        "\n",
        "    def extract_topic(self, text):\n",
        "        \"\"\"\n",
        "        Extracts the topic from the given text using simple heuristics.\n",
        "        \"\"\"\n",
        "        sentences = text.split(\". \")\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            if len(words) > 3:\n",
        "                return \" \".join(words[:4])  # Extract first few words as a topic approximation\n",
        "        return \"General Topic\"\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        \"\"\"\n",
        "        Analyzes sentiment of the translated text.\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            scores = outputs.logits.cpu().numpy()[0]\n",
        "            scores = softmax(scores)\n",
        "\n",
        "        sentiment = self.labels[scores.argmax()]\n",
        "        confidence = round(scores.max(), 2)\n",
        "        topic = self.extract_topic(text)\n",
        "\n",
        "        if sentiment == \"Positive\":\n",
        "            inference = f\"The sentiment around '{topic}' is positive, suggesting a favorable view.\"\n",
        "        elif sentiment == \"Negative\":\n",
        "            inference = f\"The sentiment around '{topic}' is negative, indicating concerns or criticisms.\"\n",
        "        else:\n",
        "            inference = f\"The sentiment around '{topic}' is mixed, with both positive and negative perspectives present.\"\n",
        "\n",
        "        return {\n",
        "            \"sentiment\": sentiment,\n",
        "            \"confidence\": confidence,\n",
        "            \"inference\": inference,\n",
        "            \"scores\": {self.labels[i]: round(float(scores[i]), 2) for i in range(len(scores))}\n",
        "        }\n",
        "\n",
        "# Example usage (for testing)\n",
        "if __name__ == \"__main__\":\n",
        "    processor = TextProcessor()\n",
        "    sample_text = input(\"Enter text: \")\n",
        "    translation_result = processor.detect_and_translate(sample_text)\n",
        "    sentiment_result = processor.analyze_sentiment(translation_result[\"translated_text\"])\n",
        "\n",
        "    print(f\"Detected Language: {translation_result['detected_language']} (Confidence: {translation_result['confidence']})\")\n",
        "    print(f\"Translated Text: {translation_result['translated_text']}\")\n",
        "    print(f\"Sentiment: {sentiment_result['sentiment']} (Confidence: {sentiment_result['confidence']})\")\n",
        "    print(f\"Inference: {sentiment_result['inference']}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jWv6FQcPBy4",
        "outputId": "93b3eb0d-070d-4627-e5f0-e16f6edfdca3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: \"ಆಡಳಿತ ಪಕ್ಷದ ಇತ್ತೀಚಿನ ನೀತಿಯಲ್ಲಿ ಬದಲಾವಣೆಗಳು ಚರ್ಚೆಗೆ ಕಾರಣವಾಗಿವೆ. ಬೆಂಬಲಿಗರು ಇದರಿಂದ ಆರ್ಥಿಕ ವೃದ್ಧಿ ಹೆಚ್ಚುತ್ತದೆ ಎಂದು ಹೇಳುತ್ತಾರೆ, ಆದರೆ ಟೀಕಾಕಾರರು ಅಸಮಾನತೆ ಹೆಚ್ಚುತ್ತದೆ ಎಂದು ವಾದಿಸುತ್ತಾರೆ. ಪ್ರಮುಖ ನಗರಗಳಲ್ಲಿ ಪ್ರತಿಭಟನೆಗಳು ಯೋಜಿತವಾಗಿವೆ.\"\n",
            "Detected Language: kn (Confidence: 1.0)\n",
            "Translated Text: \"Changes in the recent policy of the ruling party have led to debate. Supporters say that this will increase economic growth, but critics argue that inequality is increasing. Protests are planned in major cities.\"\n",
            "Sentiment: Neutral (Confidence: 0.5799999833106995)\n",
            "Inference: The sentiment around '\"Changes in the recent' is mixed, with both positive and negative perspectives present.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference drawer!!!!!\n"
      ],
      "metadata": {
        "id": "mlPVd30B6tUF"
      }
    }
  ]
}